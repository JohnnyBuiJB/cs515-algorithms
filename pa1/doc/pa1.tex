\documentclass[12pt,article]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2cm, right=2cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{mdframed}
\usepackage{changepage}   % for the adjustwidth environment
\usepackage{forest} 
\usepackage{tikz}   % For graph

\usepackage{float}  % To inforce inserting images at the right place

\usepackage{algorithm}
\usepackage{algpseudocode}

% For recursive formulation, adapted from https://tex.stackexchange.com/questions/580333/typing-sequences-recursively-in-overleaf
\usepackage{mathtools}
\makeatletter
\newcases{centercases}{\quad}
  {\hfil$\m@th\displaystyle{##}$\hfil}
  {$\m@th\displaystyle{##}$\hfil}{\lbrace}{.}
\makeatother

\newcommand{\Tau}{\mathrm{T}}


% For matrix
\def\horzbar{\text{magic}}

\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

\newcommand\projnumber{1}
\newcommand\course{CS534}
\newcommand\OSUID{934370552}
\newcommand\Email{buivy@oregonstate.edu}
\newcommand\Name{Vy Bui}
\newcommand\tab[1][1cm]{\hspace*{#1}}

\pagestyle{fancyplain}
\headheight 35pt
\lhead{Practice Assignment \projnumber}
\rhead{Jan. 19, 2023}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\newenvironment{problem}[2][Problem]
    { \begin{mdframed}[backgroundcolor=gray!20] \textbf{#1 #2} \\}
    {  \end{mdframed}}
   

% Make Rightarrow with superscript
% \makeatletter
% \newcommand{\xRightarrow}[2][]{\ext@arrow 0359\Rightarrowfill@{#1}{#2}}
% \makeatother

\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{4cm}

        \textbf{\Large CS515 - Algorithms \& Data Structures}

        \vspace{0.5cm}
 
        \textbf{\Large Practice Assignment 1}
 
        \vspace{1cm}

        Vy Bui - 934370552

        \vspace{2cm}

        Instructor: Professor Glencora Borradaile
        \vfill
             
        \vspace{0.8cm}
      
             
        The School of Electrical Engineering and Computer Science\\
        Oregon State University\\
             
    \end{center}
\end{titlepage}

%==============================================================================%
\begin{problem}{1} 
Suppose you are given an array A[1..n] of integers, which may be positive, negative, or zero. Describe a linear-time (i.e. O(n)-time) algorithm that finds the largest sum of elements in a contiguous, nonempty subarray A[i..j]. For example, given the array [-6, 12,-7, 0, 14,-7, 5] as input, your algorithm should return the integer 19 (the sum of [12,-7, 0, 14]).

For the sake of analysis, assume that comparing, adding, or multiplying any pair of numbers takes O(1) time.

\end{problem}

\textbf{Recursive Formulation}
Let $LSA(k)$ denote the function that can find the subarray of $A[k:n]$ with the largest sum of its elements. $LSA$ returns a pair of the sum of the largest subarray and its prefix sum in the aforementioned order. For example, let $A = [-1,2,1]$, $LSA(1) = (3,-1)$, with the two last elements as the largest subarray and the first element as the prefix. 


$LSA$ can be defined recursively as follows:

\tiny{
    \[
        LSA(k)=
        \begin{centercases}
            (0,0)               & k > n \land k < 1 \\
            (A[k],0)         & A[k] > LSA(k+1)[0] \land LSA(k+1)[0] < -LSA(k+1)[1] \\
            (A[k] + LSA(k+1)[0] + LSA(k+1)[1],0) & A[k] \geq -LSA(k+1)[1] \land LSA(k+1)[0] \geq -LSA(k+1)[1] \\
            (LSA(k+1)[0], A[k] + LSA(k+1)[1]) & A[k] < -LSA(k+1)[1] \land A[k] < LSA(k+1)[0] \\
        \end{centercases}
        \]
}

\normalsize{}

\textbf{Proof/Explanation}

For simplicity, in this section, let $LSA(k)$ denote the subarray of interest only, but not its prefix. This problem's optimal substructure property can be described as follows. The optimal solution to the problem with $A[k:n]$ can be found based on the optimal solution to the problem with $A[k+1:n]$. In particular, assume that we know the optimal solution to the problem of $A[k+1:n]$, adding $A[k]$ to the problem results in two new candidate largest subarrays. First, $A[k]$ can be combined with $LSA(k+1)$ to create a new solution. Note that this comes with the cost of the sum of the prefix of $LSA(k+1)$ because the new solution has to be contiguous. The second candidate is $A[k]$ itself. $LSA(k)$ has the largest sum among these two new candidates and $LSA(k+1)$. Note that the combination solution is prioritized when a tie happens in order to open up opportunities for further combination.

We can use proof by contradiction to prove this algorithm. Assume that there exists a better solution $LSA'(k)$ that is not one of the three candidates, $A[k]$, the combination, and $LSA(k+1)$. First, because $LSA'(k)$ is not $A[k]$, it must be a subarray of $A[k+1:n]$. This makes $LSA'(1)$ the subarray of the largest sum of $A[k+1:n]$ because $sum(LSA'(k)) > sum(LSA(k+1))$ based on our assumption. However, this contradicts another assumption that $LSA(2)$ is the optimal solution to the problem with $A[2:n]$. The proof completes!

\newpage
\textbf{Pseudocode}

Observe that $LSA(k)$ only depends on $LSA(k+1)$, we can implement this algorithm iteratively from $LSA(n)$ to $LSA(0)$. The solution is $LSA(0)$.


\begin{algorithm}
\caption{$LSA(A[1:n])$}\label{alg:LSA}
\begin{algorithmic}
    \State $last\_prefix\_sum \gets 0$\
    \State $last\_largest\_sum \gets 0$\
    \State $k \gets n$\;
    
    \While{$k \geq 1$}
        \State $combined\_sum \gets A[k] + last\_largest\_sum + last\_prefix\_sum$

        \State $max\_largest\_sum \gets max(A[k], last\_largest\_sum, combined\_sum)$\

        \If{$combined\_sum == max\_largest\_sum$}
            \State $last\_prefix\_sum \gets 0$\
            \State $last\_largest\_sum \gets combined\_sum$\
        \ElsIf{$A[k] == max\_largest\_sum$}
            \State $last\_prefix\_sum \gets 0$\
            \State $last\_largest\_sum \gets A[k]$\
        \ElsIf{$last\_largest\_sum == max\_largest\_sum$}
            \State $last\_prefix\_sum \gets last\_prefix\_sum + A[k]$\
        \EndIf

        \State $k \gets k - 1$
    \EndWhile

    \Return $last\_largest\_sum$
\end{algorithmic}
\end{algorithm}

\textbf{Runing Time and Space Analysis}
There are n iteration, each of which has constant number of operations, hence the algorithm has $O(n)$ time complexity. Furthermore, the algorithm uses $O(1)$ space.
%==============================================================================%
\newpage
\begin{problem}{2} 
String A is a supersequence of string B if string B can be obtained from string A by removing letters. For example, the strings BARNYARDSNACK, YUMMYBANANAS, and BWANWANWA are supersequences of the string BANANA. Give a dynamic program for finding the length of the shortest string that is a supersequence of two input strings A and B.
\end{problem}

We can create the shortest supersequence of A and B by modifying one of them to make it become a supersequence of the other with the least modifications. To keep the resulted sequence a supersequence of the starting string, only insertions are allowed. The proof will be given in the subsequent section. 

The problem now becomes finding the least number of insertions to turn one string to another. This resembles the Edit Distance problem mentioned in \cite{JeffE19} but simpler because it only allows insertions. Intuitively, starting with a longer string leads to less insertions, but both ways will result an optimal solution.

\textbf{Recursive Formulation}

Let $MI(i,j)$ denote the program that can return the minimum of insertions to transform $A[i:n]$ to a supersequence of $B[j:m]$. 

\normalsize{
    \[
        MI(i,j)=
        \begin{centercases}
            j   & i = 0 \\
            0   & j = 0 \\
            min(MI(i,j+1) + 1, MI(i+1,j+1)) & otherwise
        \end{centercases}
        \]
}

\textbf{Proof/Explanation}

Assume that we know the minimum number of insertions k needed to transform A to a supersequence S of B. S will be also a supersequence of A because we can always remove the inserted letters to transform it back to A. To prove that S is the shortest supersequence of A and B, proof by contradiction is used. Assume that there exists a shorter supersequence R of A and B. We can always transform A to R by only inserting some m letters into A. Because R is shorter than S, m is shorter than k, which contradicts with the assumption that k is the minimum number of insertions needed to transform A to a supersequence of B. Therefore, R does not exist and S is the shortest supersequence of A and B.

\textbf{Pseudocode}

\textbf{Runing Time Analysis}

%==============================================================================%
\newpage
\begin{problem}{3} 
Find the length (number of edges) of the longest path in a binary tree.
\end{problem}

\textbf{Observation 1:} A binary tree T includes two subtrees L and R, and the root node. The longest path of T, LP(T) can be a path within L, a path within R (2), or a combination of the longest path from the root node to some node in L and the longest path from the root node to some node in R (3). The longest path from some node in L to the root node is the depth of L + 1, and the same applies to R.

\textbf{Observation 2:} The depth of tree T, D(T), is the largest of the depth of the left subtree and the depth of the right subtree.

\textbf{Recursive Formulation}
\normalsize{
\[
    D(T)=
    \begin{centercases}
        -1   & T = \emptyset \\
        0    & T.L = \emptyset \land T.R = \emptyset \\
        1 + max(D(L), D(R)) & otherwise
    \end{centercases}
\]
}

\normalsize{
\[
    LP(T)=
    \begin{centercases}
        -1   & T = \emptyset \\
        0    & T.L = \emptyset \land T.R = \emptyset \\
        max(LP(L), LP(R), D(L) + D(R) + 2) & otherwise
    \end{centercases}
\]
}

\textbf{Proof/Explanation}

We will prove that there not exist a path in T that is longer than LP(L), LP(R), and D(L) + D(R) + 2, using proof by contradiction. Assume that there exists a longer path $(x,y)$ with $x,y \in L \cup R \cup r$ where $r$ is the root node. If $x,y \in L$, then it contradicts with the assumption that LP(L) is the longest path in L, which shows that such path does not exist. The same argument can be applied to path within the right tree. Lastly, if $x \in L$ and $y \in R$ then it has to go through r. We can break the path into two pieces, $(x,r)$ and $(r,y)$ so that $|(x,y)| = |(x,r)| + |(r,y)|$, which can not be greater than D(L) + D(R). Thus, such a path does not exist.

\newpage
\textbf{Pseudocode}
For this problem, a recursive implementation seems to be more efficient, readable, and easier to implement compared to an iterative implementation.

\begin{algorithm}
\caption{$D(T)$}\label{alg:D}
\begin{algorithmic}
    \If{$T == \emptyset$}
        \Return -1
    \EndIf

    \If{$T.L == \emptyset \land T.R == \emptyset$}
        \Return 0
    \EndIf

    \Return $1 + max(D(T.L), D(T.R))$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{$LP(T)$}\label{alg:LP}
\begin{algorithmic}
    \If{$T == \emptyset$}
        \Return -1
    \EndIf

    \If{$T.L == \emptyset \land T.R == \emptyset$}
        \Return 0
    \EndIf

    \Return $max(LP(T.L), LP(T.R), D(T.L) + D(T.R) + 2)$
\end{algorithmic}
\end{algorithm}

\textbf{Runing Time Analysis}

The iterative implementation needs to compute LP from the bottom up, that is from the leaves up to the root. The problem is we do not know the leaves in advance so it needs to traverse the trees one more time to find the leaves. For memory, it needs O(n) of memory to keep the results in the worst case that the tree is a full binary tree, which has (n + 1) / 2 leaves. On the other hand, a recursive algorithm only needs to traverse through the tree once because a node depends on exactly its two children nodes. Therefore, it takes O(n) time and O(n) space of stack to store the recursive calls.

%==============================================================================%
\newpage
\begin{problem}{4} 
Suppose you are given an n x n bitmap, represented by a 2-dimensional array M[1..n, 1..n] of 0s and 1s. A solid block in M is a subarray of the form M[i..i', j..j'] containing only 1s. Describe an algorithm to find the area of the maximum solid block in M in $O(n^3)$ time. If you can do that, try to design a faster algorithm that runs in $O(n^2)$ time.
\end{problem}

\textbf{Recursive Formulation}

\textbf{Proof/Explanation}

\textbf{Pseudocode}

\textbf{Runing Time Analysis}

\bibliographystyle{alpha}
\bibliography{mybib}
\end{document}
