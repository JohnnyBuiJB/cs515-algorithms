\documentclass[12pt,article]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2cm, right=2cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{mdframed}
\usepackage{changepage}   % for the adjustwidth environment
\usepackage{forest} 
\usepackage{tikz}   % For graph

\usepackage{float}  % To inforce inserting images at the right place

\usepackage{algorithm}
\usepackage{algpseudocode}

% For recursive formulation, adapted from https://tex.stackexchange.com/questions/580333/typing-sequences-recursively-in-overleaf
\usepackage{mathtools}
\makeatletter
\newcases{centercases}{\quad}
  {\hfil$\m@th\displaystyle{##}$\hfil}
  {$\m@th\displaystyle{##}$\hfil}{\lbrace}{.}
\makeatother

\newcommand{\Tau}{\mathrm{T}}


% For matrix
\def\horzbar{\text{magic}}

\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

\newcommand\projnumber{1}
\newcommand\course{CS534}
\newcommand\OSUID{934370552}
\newcommand\Email{buivy@oregonstate.edu}
\newcommand\Name{Vy Bui}
\newcommand\tab[1][1cm]{\hspace*{#1}}

\pagestyle{fancyplain}
\headheight 35pt
\lhead{Class Notes}
% \rhead{Jan. 19, 2023}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\newenvironment{problem}[2][Problem]
    { \begin{mdframed}[backgroundcolor=gray!20] \textbf{#1 #2} \\}
    {  \end{mdframed}}
   

% Make Rightarrow with superscript
% \makeatletter
% \newcommand{\xRightarrow}[2][]{\ext@arrow 0359\Rightarrowfill@{#1}{#2}}
% \makeatother

\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{4cm}

        \textbf{\Large CS515 - Algorithms \& Data Structures}

        \vspace{0.5cm}
 
        \textbf{\Large Notes}
 
        \vspace{1cm}

        Vy Bui - 934370552

        \vspace{2cm}

        Instructor: Professor Glencora Borradaile
        \vfill
             
        \vspace{0.8cm}
      
             
        The School of Electrical Engineering and Computer Science\\
        Oregon State University\\
             
    \end{center}
\end{titlepage}


%==============================================================================%
\section{Divide and Conquer}
\subsection{Pattern}

There are 3 strategies:

A. Simply divide input to 2 equal parts, then recurse on each side and combine the solutions (merge sort, binary search).

B. \textbf{Find a clever way} to reduce the number of subproblems so that we can bypass some of the subproblems. For example, in binary search, at each iteration, we only consider a half of the problem and circumvent the other half. This helps save lots of time.

C. \textbf{Find something useful} for dividing the input. For example, quick sort relies on 

%==============================================================================%
\subsection{Proof Skeleton}



%==============================================================================%
\subsection{Common Recurrence}

Merge sort: $$T(n) = 2T(n/2) + O(n) = O(nlogn)$$

Integer Multiplication: 
$$T(n) = 3T(\frac{n}{2}) + O(n) = O(n^{log_23}) = O(n^{1.59})$$

Median Selection: 
$$T(n) = T(\frac{n}{5}) + T(\frac{7n}{10}) + O(n) = T(\frac{9n}{10}) + O(n) = O(n)$$


%==============================================================================%
\newpage
\section{Dynamic Programming}
\subsection{Example Problems}

%==============================================================================%
\begin{problem}{1}
\textbf{Longest Increasing Sequence}: Given an array of integers, find the longest increasing sequence. 

Example 1: $LIS([6,3,9,12,4,7,10,3]) = len([3,4,7,10]) = 4$

Example 2: $LIS([7,8,9,2,3]) = len([7,8,9]) = 3$

\end{problem}

\textbf{Recursive Formulation}

\tiny{
    \[
        LIS(k)=
        \begin{centercases}
            0               & k < 0 \\
            1               & k = 0 \\
            max()                & otherwise \\
        \end{centercases}
        \]
}

\normalsize{}

\textbf{Proof/Explanation}

For simplicity, in this section, let $LSA(k)$ denote the subarray of interest only, but not its prefix. This problem's optimal substructure property can be described as follows. The optimal solution to the problem with $A[k:n]$ can be found based on the optimal solution to the problem with $A[k+1:n]$. In particular, assume that we know the optimal solution to the problem of $A[k+1:n]$, adding $A[k]$ to the problem results in two new candidate largest subarrays. First, $A[k]$ can be combined with $LSA(k+1)$ to create a new solution. Note that this comes with the cost of the sum of the prefix of $LSA(k+1)$ because the new solution has to be contiguous. The second candidate is $A[k]$ itself. $LSA(k)$ has the largest sum among these two new candidates and $LSA(k+1)$. Note that the combination solution is prioritized when a tie happens in order to open up opportunities for further combination.

We can use proof by contradiction to prove this algorithm. Assume that there exists a better solution $LSA'(k)$ that is not one of the three candidates, $A[k]$, the combination, and $LSA(k+1)$. First, because $LSA'(k)$ is not $A[k]$, it must be a subarray of $A[k+1:n]$. This makes $LSA'(1)$ the subarray of the largest sum of $A[k+1:n]$ because $sum(LSA'(k)) > sum(LSA(k+1))$ based on our assumption. However, this contradicts another assumption that $LSA(2)$ is the optimal solution to the problem with $A[2:n]$. The proof completes!

\newpage
\textbf{Pseudocode}

Observe that $LSA(k)$ only depends on $LSA(k+1)$, we can implement this algorithm iteratively from $LSA(n)$ to $LSA(0)$. The solution is $LSA(0)$.


\begin{algorithm}
\caption{$LSA(A[1:n])$}\label{alg:LSA}
\begin{algorithmic}
    \State $last\_prefix\_sum \gets 0$\
    \State $last\_largest\_sum \gets 0$\
    \State $k \gets n$\;
    
    \While{$k \geq 1$}
        \State $combined\_sum \gets A[k] + last\_largest\_sum + last\_prefix\_sum$

        \State $max\_largest\_sum \gets max(A[k], last\_largest\_sum, combined\_sum)$\

        \If{$combined\_sum == max\_largest\_sum$}
            \State $last\_prefix\_sum \gets 0$\
            \State $last\_largest\_sum \gets combined\_sum$\
        \ElsIf{$A[k] == max\_largest\_sum$}
            \State $last\_prefix\_sum \gets 0$\
            \State $last\_largest\_sum \gets A[k]$\
        \ElsIf{$last\_largest\_sum == max\_largest\_sum$}
            \State $last\_prefix\_sum \gets last\_prefix\_sum + A[k]$\
        \EndIf

        \State $k \gets k - 1$
    \EndWhile

    \Return $last\_largest\_sum$
\end{algorithmic}
\end{algorithm}

\textbf{Runing Time and Space Analysis}
There are n iteration, each of which has constant number of operations, hence the algorithm has $O(n)$ time complexity. Furthermore, the algorithm uses $O(1)$ space.

%==============================================================================%
\newpage
\section{Greedy Algorithms}
\subsection{Greedy Stays Ahead}
Let A be the algorithm's solution and O be the optimal solution. 

1. Align A and O in some ways so that we can compare $a_i$ with $o_i$. \\
2. Use proof by induction to prove that $a_i$ is better than or of the same quality as $o_i$. \\
3. Use proof by contradiction and (2) to prove the algorithm will eventually lead to the optimal solution, starting off with assume that it won't and come to an contradiction.

Refer to section 4.1 in \cite{kleinbergalgorithm} for more details.
%==============================================================================%
\newpage
\subsection{An Exchange Argument}

%==============================================================================%
\newpage
\subsection{Matroids}
Matching Matroid

Disjoint Path Matroid: G = (V,E) + fixed vertex $S \subset V$

\subsubsection{Cases where Matroids fail}

Theorem X (\cite{JeffE19}, appendix E): For any family of subsets I that do not form a matroid, there exists a weight function for which greedy fails.

Greedy fails on the \textbf{Set Cover Problem}: give a family I of subsets of S. Find that smallest subset of I that covers everything in S. 

\subsubsection{Reference}
16.4, \cite{CLRS}


%==============================================================================%
\newpage
\section{Randomized Algorithms}

\subsection{Definition}

\subsection{Application}
1. Using randomization to improve run time \\
2. Guarantee the right solution but run time will be a random variable (Quicksort) \\
3. Allowed to make mistakes with low probability \\
4. Efficient testing (tesing sortedness)

\subsection{Math}

\subsubsection{Arithmetic Series}

$$a_k = a_0 + nd$$

$$\sum^{n}_{i=0}a_i = \frac{n(a_0 + a_n)}{2}$$

\subsubsection{Geometric Series}
The name geometric series indicates each term is the geometric mean of its two neighboring terms, similar to how the name arithmetic series indicates each term is the arithmetic mean of its two neighboring terms.

$$a_k = a_0 r^k$$

$$\sum^{n}_{i=0}a_i = 1 + r + r^2 + ... + r^n = \frac{1 - r^{n+1}}{1 - r}$$

For $-1 < r < 1$, the sum converges as $a \rightarrow \infty$, 
$$\sum^{\infty}_{i=0}a_i = \frac{1}{1 - r}$$

\subsubsection{Arithmetico-geometric Series}

An arithmetic-geometric progression (AGP) is a progression in which each term can be represented as the product of the terms of an arithmetic progressions (AP) and a geometric progressions (GP).
$$\frac{1}{2} + \frac{2}{4} + \frac{3}{8} + \frac{5}{32} + ... = ?$$

The sequence has the form
$$a_n = (a + (n-1)d)r^{n-1}$$

$$S_{\infty} = \sum^n_{k=1} kr^k = \frac{r}{(1-r)^2}$$

\subsubsection{Harmonic number}
Harnomic series has the form
$$H(n) = \sum_{i=1}^{n} \frac{1}{i}$$

According to \cite{kleinbergalgorithm}, harmonic number is bounded by $\Theta(logn)$
$$ln(n + 1) < H(n) < 1 + ln(n)$$


\subsection{Examples}

\begin{problem}{Randomized algorithm for global min cut} 
Input: undirected G=(V,E)\\
Find a min sized subset $C \subset E$ such that $G = (V,E - C)$ is disconnected.
\end{problem}

\textbf{Edge Contraction Operation}: ???

If you contract $E - C$, then your graph looks like 2 nodes with many edges connecting them, how???

\begin{problem}{Sortedness Testing} 
Given a array A of n items, create randomized an algorithm than can check if A has a 99\% chance to be sorted in O(logn) time
\end{problem}

To say with probability of 99\% if A is nearly sorted: ignoring an $\epsilon$ fraction of elements, rest are sorted.

Idea 1: pick random indices i and j and compare $a_i$ an $a_j$.

This won't work in case that A = 1,1,1,1,1,1,0,0,0,0,0,0. A is not nearly sorted.

p(success) = k/n for k tests. For P(success) > 0.99, need O(n) tests.

Idea 2: pick 2 random indices $i \leq j$ and say that A not sorted if $x_i > x_j$.

This won't work in case A = [1,0,2,1,3,2,4,4,5], that has a subset of even-indexed numbers or a subset of odd-indexed numbers sorted. We will say A is sorted uness we pick i odd and j = i + 1.

P(success) = low ???
\\
\\
\textbf{Algorithm 2}: \\
For k iterations: \\
\tab[1cm] Pick 2 items from A \\
\tab[1cm] If they are not in sorted order \\
\tab[1cm]   output "not sorted" \\
\tab[1cm] Else \\ 
\tab[1cm]   continue \\ 

Refer to "Testing if an Array is sorted".


\subsubsection{Reference}
Chapter 13, \cite{kleinbergalgorithm}

%==============================================================================%
\newpage
\bibliographystyle{alpha}
\bibliography{mybib}
\end{document}
