\documentclass[12pt,article]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2cm, right=2cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{mdframed}
\usepackage{changepage}   % for the adjustwidth environment
\usepackage{forest} 
\usepackage{tikz}   % For graph

\usepackage{float}  % To inforce inserting images at the right place

\usepackage{algorithm}
\usepackage{algpseudocode}

% For recursive formulation, adapted from https://tex.stackexchange.com/questions/580333/typing-sequences-recursively-in-overleaf
\usepackage{mathtools}
\makeatletter
\newcases{centercases}{\quad}
  {\hfil$\m@th\displaystyle{##}$\hfil}
  {$\m@th\displaystyle{##}$\hfil}{\lbrace}{.}
\makeatother

\newcommand{\Tau}{\mathrm{T}}


% For matrix
\def\horzbar{\text{magic}}

\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

\newcommand\projnumber{1}
\newcommand\course{CS534}
\newcommand\OSUID{934370552}
\newcommand\Email{buivy@oregonstate.edu}
\newcommand\Name{Vy Bui}
\newcommand\tab[1][1cm]{\hspace*{#1}}

\pagestyle{fancyplain}
\headheight 35pt
\lhead{Class Notes}
% \rhead{Jan. 19, 2023}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\newenvironment{problem}[2][Problem]
    { \begin{mdframed}[backgroundcolor=gray!20] \textbf{#1 #2} \\}
    {  \end{mdframed}}

\newenvironment{theorem}[2][Theorem]
    { \begin{mdframed}[backgroundcolor=blue!10] \textbf{#1 #2} \\}
    {  \end{mdframed}}

% Make Rightarrow with superscript
% \makeatletter
% \newcommand{\xRightarrow}[2][]{\ext@arrow 0359\Rightarrowfill@{#1}{#2}}
% \makeatother

\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{4cm}

        \textbf{\Large CS515 - Algorithms \& Data Structures}

        \vspace{0.5cm}
 
        \textbf{\Large Notes}
 
        \vspace{1cm}

        Vy Bui - 934370552

        \vspace{2cm}

        Instructor: Professor Glencora Borradaile
        \vfill
             
        \vspace{0.8cm}
      
             
        The School of Electrical Engineering and Computer Science\\
        Oregon State University\\
             
    \end{center}
\end{titlepage}


%==============================================================================%
\section{Divide and Conquer}
\subsection{Pattern}

There are 3 strategies:

A. Simply divide input to 2 equal parts, then recurse on each side and combine the solutions (merge sort, binary search).

B. \textbf{Find a clever way} to reduce the number of subproblems so that we can bypass some of the subproblems. For example, in binary search, at each iteration, we only consider a half of the problem and circumvent the other half. This helps save lots of time.

C. \textbf{Find something useful} for dividing the input. For example, quick sort relies on 

%==============================================================================%
\subsection{Proof Skeleton}



%==============================================================================%
\subsection{Common Recurrence}

Merge sort: $$T(n) = 2T(n/2) + O(n) = O(nlogn)$$

Integer Multiplication: 
$$T(n) = 3T(\frac{n}{2}) + O(n) = O(n^{log_23}) = O(n^{1.59})$$

Median Selection: 
$$T(n) = T(\frac{n}{5}) + T(\frac{7n}{10}) + O(n) = T(\frac{9n}{10}) + O(n) = O(n)$$


%==============================================================================%
\newpage
\section{Dynamic Programming}
\subsection{Example Problems}

%==============================================================================%
\begin{problem}{1}
\textbf{Longest Increasing Sequence}: Given an array of integers, find the longest increasing sequence. 

Example 1: $LIS([6,3,9,12,4,7,10,3]) = len([3,4,7,10]) = 4$

Example 2: $LIS([7,8,9,2,3]) = len([7,8,9]) = 3$

\end{problem}

\textbf{Recursive Formulation}

\tiny{
    \[
        LIS(k)=
        \begin{centercases}
            0               & k < 0 \\
            1               & k = 0 \\
            max()                & otherwise \\
        \end{centercases}
        \]
}

\normalsize{}

\textbf{Proof/Explanation}

For simplicity, in this section, let $LSA(k)$ denote the subarray of interest only, but not its prefix. This problem's optimal substructure property can be described as follows. The optimal solution to the problem with $A[k:n]$ can be found based on the optimal solution to the problem with $A[k+1:n]$. In particular, assume that we know the optimal solution to the problem of $A[k+1:n]$, adding $A[k]$ to the problem results in two new candidate largest subarrays. First, $A[k]$ can be combined with $LSA(k+1)$ to create a new solution. Note that this comes with the cost of the sum of the prefix of $LSA(k+1)$ because the new solution has to be contiguous. The second candidate is $A[k]$ itself. $LSA(k)$ has the largest sum among these two new candidates and $LSA(k+1)$. Note that the combination solution is prioritized when a tie happens in order to open up opportunities for further combination.

We can use proof by contradiction to prove this algorithm. Assume that there exists a better solution $LSA'(k)$ that is not one of the three candidates, $A[k]$, the combination, and $LSA(k+1)$. First, because $LSA'(k)$ is not $A[k]$, it must be a subarray of $A[k+1:n]$. This makes $LSA'(1)$ the subarray of the largest sum of $A[k+1:n]$ because $sum(LSA'(k)) > sum(LSA(k+1))$ based on our assumption. However, this contradicts another assumption that $LSA(2)$ is the optimal solution to the problem with $A[2:n]$. The proof completes!

\newpage
\textbf{Pseudocode}

Observe that $LSA(k)$ only depends on $LSA(k+1)$, we can implement this algorithm iteratively from $LSA(n)$ to $LSA(0)$. The solution is $LSA(0)$.


\begin{algorithm}
\caption{$LSA(A[1:n])$}\label{alg:LSA}
\begin{algorithmic}
    \State $last\_prefix\_sum \gets 0$\
    \State $last\_largest\_sum \gets 0$\
    \State $k \gets n$\;
    
    \While{$k \geq 1$}
        \State $combined\_sum \gets A[k] + last\_largest\_sum + last\_prefix\_sum$

        \State $max\_largest\_sum \gets max(A[k], last\_largest\_sum, combined\_sum)$\

        \If{$combined\_sum == max\_largest\_sum$}
            \State $last\_prefix\_sum \gets 0$\
            \State $last\_largest\_sum \gets combined\_sum$\
        \ElsIf{$A[k] == max\_largest\_sum$}
            \State $last\_prefix\_sum \gets 0$\
            \State $last\_largest\_sum \gets A[k]$\
        \ElsIf{$last\_largest\_sum == max\_largest\_sum$}
            \State $last\_prefix\_sum \gets last\_prefix\_sum + A[k]$\
        \EndIf

        \State $k \gets k - 1$
    \EndWhile

    \Return $last\_largest\_sum$
\end{algorithmic}
\end{algorithm}

\textbf{Runing Time and Space Analysis}
There are n iteration, each of which has constant number of operations, hence the algorithm has $O(n)$ time complexity. Furthermore, the algorithm uses $O(1)$ space.

%==============================================================================%
\newpage
\section{Greedy Algorithms}
\subsection{Greedy Stays Ahead}
Let A be the algorithm's solution and O be the optimal solution. The idea is to "Compare the partial solutions that the greedy algorithm constructs to initial segments of O, and show that the greedy algorithm is doing better in a step-by-step fashion", \cite{kleinbergalgorithm}.

Steps: \\
1. Align A and O in some ways so that we can compare $a_i$ with $o_i$. \\
2. Use proof by induction to prove that $a_i$ is better than or of the same quality as $o_i$. \\
3. Use proof by contradiction and (2) to prove the algorithm will eventually lead to the optimal solution, starting off with the assumption that it won't and come to an contradiction.

Refer to section 4.1 in \cite{kleinbergalgorithm} for more details.

\begin{problem}{Interval Scheduling Problem} 
Input: a list of intervals I = \{(s,e)\} with s and e being start and end time respectively. Two intervals overlap when they occur at the same time, excluding start and end time. For example, (1,2) does not overlap (2,3). \\

Problem: Find the maximum number of non-overlapping intervals in I.
\end{problem}

\textbf{Greedy Algorithm}\\
1. Sort I by start time.\\
2. Iterate through I, add the interval to the solution if it does not overlap with any intervals. If some intervals overlap, pick the one with the smallest end time.

Let $A = a_1, ..., a_k$ be the solution constructed by the greedy algorithm and $O = o_1, ..., o_m$ be the optimal solution, both of which are sorted by end time. We will prove that $k >= m$.

\textbf{Lemma 1}. $a_i.e < o_i.e$ for any $i \leq k$.\\
\textbf{Proof.} We prove this by induction. For base case $i = 1$, the statement is clearly true because the algorithm picks that smallest end time.

Induction hypothesis: $a_i.e < o_i.e$ for any $i > 1$.

Induction step: $a_{i+1}.e < o_{i+1}.e$? This is always true because either $a_{i+1}.e$ or $o_{i+1}.e$ is compatible with $A$, therefore we can always pick $o_{i+1}$ if it has smaller end time. Proof completes!

\textbf{Theorem}. The greedy algorithm returns an optimal set A.\\
\textbf{Proof.} Assume that A is not optimal, followed by $m > k$. Since $m > k$, there must be some $o_{k+1}$ in O that starts after $o_{k}.e$, and hence after $a_{k}.e$ (Lemma 1). Therefor, $o_{k+1}$ is compatible with the current set A, and the algorithm would have picked it. This contradicts the assumption that A is not optimal. Proof completes!

\begin{problem}{Minimum interval removals for non-overlapping} 
Input: a list of intervals I = \{(s,e)\} with s and e being start and end time respectively. Two intervals overlap when they occur at the same time, excluding start and end time. For example, (1,2) does not overlap (2,3). \\

Problem: Find the minimum number of intervals in I that need to be removed so that I is non-overlapping.
\end{problem}

\textbf{Greedy Algorithm}\\
1. Sort I by end time.\\
2. Iterate through I. If the interval overlaps with the previous interval, remove the one with later end time. Otherwise, keep it. \\

%==============================================================================%
\newpage
\subsection{An Exchange Argument}
Let A be the algorithm's solution and O be the optimal solution. Gradually modify O, \textbf{preserving its optimality} at each step, but eventually transforming it into A.

1. Derive some useful properties from the greedy choice \\
2. Assume that there exists an $O$ solution without these properties. Gradually tweak $O$ until it has these properties. Prove that each tweak results in another optimal solution.

%==============================================================================%
\newpage
\subsection{Matroids}
Matching Matroid

Disjoint Path Matroid: G = (V,E) + fixed vertex $S \subset V$

\subsubsection{Cases where Matroids fail}

Theorem X (\cite{JeffE19}, appendix E): For any family of subsets I that do not form a matroid, there exists a weight function for which greedy fails.

Greedy fails on the \textbf{Set Cover Problem}: give a family I of subsets of S. Find that smallest subset of I that covers everything in S. 

\subsubsection{Reference}
16.4, \cite{CLRS}


%==============================================================================%
\newpage
\section{Randomized Algorithms}

\subsection{Definition}

\subsection{Application}
1. Using randomization to improve run time \\
2. Guarantee the right solution but run time will be a random variable (Quicksort) \\
3. Allowed to make mistakes with low probability \\
4. Efficient testing (tesing sortedness)

\subsection{Math}

\subsubsection{Arithmetic Series}

$$a_k = a_0 + nd$$

$$\sum^{n}_{i=0}a_i = \frac{n(a_0 + a_n)}{2}$$

\subsubsection{Geometric Series}
The name geometric series indicates each term is the geometric mean of its two neighboring terms, similar to how the name arithmetic series indicates each term is the arithmetic mean of its two neighboring terms.

$$a_k = a_0 r^k$$

$$\sum^{n}_{i=0}a_i = 1 + r + r^2 + ... + r^n = \frac{1 - r^{n+1}}{1 - r}$$

For $-1 < r < 1$, the sum converges as $a \rightarrow \infty$, 
$$\sum^{\infty}_{i=0}a_i = \frac{1}{1 - r}$$

\subsubsection{Arithmetico-geometric Series}

An arithmetic-geometric progression (AGP) is a progression in which each term can be represented as the product of the terms of an arithmetic progressions (AP) and a geometric progressions (GP).
$$\frac{1}{2} + \frac{2}{4} + \frac{3}{8} + \frac{5}{32} + ... = ?$$

The sequence has the form
$$a_n = (a + (n-1)d)r^{n-1}$$

$$S_{\infty} = \sum^{\infty}_{k=1} kr^k = \frac{r}{(1-r)^2}$$

\subsubsection{Harmonic number}
Harnomic series has the form
$$H(n) = \sum_{i=1}^{n} \frac{1}{i}$$

According to \cite{kleinbergalgorithm}, harmonic number is bounded by $\Theta(logn)$
$$ln(n + 1) < H(n) < 1 + ln(n)$$

\subsubsection{Probability}

\begin{theorem}{13.7}
If we repeatedly perform independent trials of an experiment, each of
which succeeds with probability $p > 0$, then the expected number of trials we
need to perform until the first success is 1/p \cite{kleinbergalgorithm}.
    
\end{theorem}

\begin{theorem}{13.8}
Linearity of Expectation. Given two random variables X and Y defined
over the same probability space, we can define X + Y to be the random variable
equal to $X(\omega) + Y(\omega)$ on a sample point $\omega$. For any X and Y, we have $E [X + Y]= E [X]+ E [Y]$ \cite{kleinbergalgorithm}.
\end{theorem}

\subsection{Examples}

\begin{problem}{Randomized algorithm for global min cut} 
Input: undirected G=(V,E)\\
Find a min sized subset $C \subset E$ such that $G = (V,E - C)$ is disconnected.
\end{problem}

\textbf{Edge Contraction Operation}: ???

If you contract $E - C$, then your graph looks like 2 nodes with many edges connecting them, how???

\begin{problem}{Sortedness Testing} 
Given a array A of n items, create randomized an algorithm than can check if A has a 99\% chance to be sorted in O(logn) time
\end{problem}

To say with probability of 99\% if A is nearly sorted: ignoring an $\epsilon$ fraction of elements, rest are sorted.

Idea 1: pick random indices i and j and compare $a_i$ an $a_j$.

This won't work in case that A = 1,1,1,1,1,1,0,0,0,0,0,0. A is not nearly sorted.

p(success) = k/n for k tests. For P(success) > 0.99, need O(n) tests.

Idea 2: pick 2 random indices $i \leq j$ and say that A not sorted if $x_i > x_j$.

This won't work in case A = [1,0,2,1,3,2,4,4,5], that has a subset of even-indexed numbers or a subset of odd-indexed numbers sorted. We will say A is sorted uness we pick i odd and j = i + 1.

P(success) = low ???
\\
\\
\textbf{Algorithm 2}: \\
For k iterations: \\
\tab[1cm] Pick 2 items from A \\
\tab[1cm] If they are not in sorted order \\
\tab[1cm]   output "not sorted" \\
\tab[1cm] Else \\ 
\tab[1cm]   continue \\ 

Refer to "Testing if an Array is sorted".


\subsubsection{Reference}
Chapter 13, \cite{kleinbergalgorithm}

%==============================================================================%
\newpage
\bibliographystyle{alpha}
\bibliography{mybib}
\end{document}
